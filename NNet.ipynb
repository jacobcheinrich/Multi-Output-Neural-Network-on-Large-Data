{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt  \n",
    "import gzip\n",
    "import os\n",
    "import tensorflow as tf\n",
    "import keras\n",
    "os.chdir('') ##set to directory"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Read in data"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "def parse(path):\n",
    "    #open a gzip-compressed file in binary read mode\n",
    "    #binary means any type of data, not just text\n",
    "    g = gzip.open(path, mode='rb') #mode: read raw binary\n",
    "    #generate records from the opened file\n",
    "    for l in g:\n",
    "        yield eval(l)"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "# Prepare dummy variable structure\n",
    "Cycle through all the data once to get all the unique words and count how many times they occur\n",
    "We will later remove words that appear few times. What is the cut off to remove items? However many items your RAM can handle.\n",
    "#We can do this with a dictionary:"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "# When trying to add a new item to the dictionary it will be added if it is not already in the dictionary. If it is already in the dictionary it will not be added.\n",
    "X_unique_words = {} #this creates an empty dictionary\n",
    "Y_unique_categories = {}"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "#counter\n",
    "i = 0\n",
    "\n",
    "## For-loop to itereate through the dataset one row at a time so that we do not overload RAM\n",
    "for d in parse('meta_Clothing_Shoes_and_Jewelry.json.gz'):\n",
    "    i += 1\n",
    "    \n",
    "    #X\n",
    "    ##############################################################\n",
    "    X = np.array(d['title'])\n",
    "    #print('\\nX (title):\\n')\n",
    "    #print(X)\n",
    "    \n",
    "    # np.char.split; split text by any whitespace. Results in numpy array.\n",
    "    # item(): make the numpy array a list\n",
    "    for j in np.char.split(X).item():\n",
    "        #add any text processing here you would like\n",
    "        #examples: \n",
    "            #remove stop words\n",
    "            #convert to lower case\n",
    "        word = j.lower()\n",
    "        if word in ['with','the','we','you','and']:\n",
    "            continue\n",
    "        # Add to dictionary:     \n",
    "            # The get() dictionary method returns the value of the item with the specified key.\n",
    "            # We can specify a value to return if the item does not exist\n",
    "            # dictionary.get(keyname, value)           \n",
    "        X_unique_words[word] = X_unique_words.get(word,0) + 1\n",
    "\n",
    "    #every 1000 iterations remove words appearing less than or equal to 5 times\n",
    "    if (i % 1000) == 0:\n",
    "        #need list around items to make copy to avoid \n",
    "        #iteration over and modifying dict at the same time\n",
    "        for key,value in list(X_unique_words.items()): \n",
    "            if value <= 5:\n",
    "                X_unique_words.pop(key)        \n",
    "        \n",
    "        \n",
    "    #Y\n",
    "    ##############################################################\n",
    "    Y = np.array(d['category'])\n",
    "    #print('\\nY (category):\\n')\n",
    "    #print(Y)                  \n",
    "\n",
    "    \n",
    "    for jj in Y:\n",
    "        category = jj.lower()\n",
    "        Y_unique_categories[category] = Y_unique_categories.get(category,0) + 1\n",
    "    \n",
    "    #every 1000 iterations remove categories appearing less than or equal to 5 times\n",
    "    if (i % 1000) == 0:\n",
    "        #need list around items to make copy to avoid \n",
    "        #iteration over and modifying dict at the same time\n",
    "        for key,value in list(Y_unique_categories.items()): \n",
    "            if value <= 5:\n",
    "                Y_unique_categories.pop(key)      \n",
    "\n",
    "    #print progress every 100k records                \n",
    "    if (i % 100000) == 0:\n",
    "        print(i)\n",
    "        \n",
    "#     if i == 1000: \n",
    "#         break                \n",
    "\n",
    "len(X_unique_words)\n",
    "len(Y_unique_categories)\n",
    "np.median(list(X_unique_words.values()))"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "#If a category is present in all items, then it is a good idea to help the model learn by removing this category.\n",
    "\n",
    "# Extract keys and assign to an ordered object type such as list. This might not be necessary anymore as dictionary is now an ordered type since Python 3.6.\n",
    "\n",
    "X_unique_words_ordered = np.array(list(X_unique_words.keys()))\n",
    "Y_unique_categories_ordered = np.array(list(Y_unique_categories))"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Build Model"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "# Create Layers\n",
    "inputs = tf.keras.layers.Input(shape=(len(uniquewords),),name='input')  # For a two dimensional input dataset, use (Nbrvariables,) for shape.\n",
    "hidden1 = tf.keras.layers.Dense(units=round(len(uniquewords)*.75), activation=\"elu\", name='hidden1')(inputs)  # number of units 20% less than input node\n",
    "hidden2 = tf.keras.layers.Dense(units=round(len(uniquewords)*.5), activation=\"elu\", name='hidden2')(hidden1)\n",
    "hidden3 = tf.keras.layers.Dense(units=round(len(uniquewords)*.25), activation=\"elu\", name='hidden3')(hidden2)\n",
    "outputs = tf.keras.layers.Dense(units=len(uniquecategories), activation=\"softplus\", name='output')(hidden3)\n",
    "# Create model\n",
    "model = tf.keras.Model(inputs=inputs, outputs=outputs)\n",
    "# Compile model\n",
    "model.compile(loss='categorical_crossentropy', optimizer=tf.keras.optimizers.SGD(learning_rate=0.001)) # opted for categorical cross entropy as opposed to binary cross entropy\n"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Train Model"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "avg_loss_per_epoch = []\n",
    "\n",
    "epochs = 5\n",
    "for epoch in range(epochs):\n",
    "    i = 0\n",
    "    avg_loss = 0\n",
    "    for d in parse('meta_Clothing_Shoes_and_Jewelry.json.gz'):\n",
    "        i += 1\n",
    "\n",
    "        #X\n",
    "        ##############################################################    \n",
    "        X = np.array(d['title'])\n",
    "\n",
    "\n",
    "        #initialize array with 0\n",
    "        X_dummies = np.zeros((1,len(X_unique_words)))\n",
    "\n",
    "        #write 1 where word appears\n",
    "        for j in np.char.split(X).item():\n",
    "            #add any text processing here you would like\n",
    "            #examples: \n",
    "                #remove stop words\n",
    "                #convert to lower case\n",
    "            word = j.lower()\n",
    "            if word in ['with','the','we','you','and']:\n",
    "                continue\n",
    "\n",
    "            #check if word appears in the lookup list (it might be an uncommon word that we deleted)\n",
    "            # if present write 1, otherwise skip to next word\n",
    "            pos = np.where(word == np.array(X_unique_words_ordered))[0]\n",
    "            if len(pos) == 1:\n",
    "                position = pos[0]\n",
    "            else:\n",
    "                continue\n",
    "\n",
    "            X_dummies[0,position] = 1\n",
    "\n",
    "        #Y\n",
    "        ##############################################################         \n",
    "        Y = np.array(d['category'])\n",
    "\n",
    "        #initialize array with 0\n",
    "        Y_dummies = np.zeros((1,len(Y_unique_categories)))\n",
    "\n",
    "        #write 1 where category appears    \n",
    "        for jj in Y:\n",
    "            category = jj.lower()\n",
    "\n",
    "            #check if category appears in the lookup list (it might be an uncommon category that we deleted)\n",
    "            # if present write 1, otherwise skip to next category            \n",
    "            pos = np.where(category == np.array(Y_unique_categories_ordered))[0]\n",
    "            if len(pos) == 1:\n",
    "                position = pos[0]\n",
    "            else:\n",
    "                continue\n",
    "\n",
    "            Y_dummies[0,position] = 1        \n",
    "\n",
    "\n",
    "        #Fit model\n",
    "        modinfo = model.fit(x=X_dummies,y=Y_dummies, batch_size=1, epochs=1, verbose=0)\n",
    "        #compute average loss\n",
    "        loss = modinfo.history['loss'][0]\n",
    "        avg_loss = avg_loss + (1/i)*(loss - avg_loss)\n",
    "        \n",
    "        #print progress every 10k records\n",
    "        if (i % 10) == 0:\n",
    "            print(i) \n",
    "        #Do 200k iterations (i.e., records)\n",
    "        if i == 2000: \n",
    "            break\n",
    "        \n",
    "    avg_loss_per_epoch.append(avg_loss)\n",
    "    plt.plot(avg_loss_per_epoch)\n",
    "    plt.show()"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Variable Importance"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "yhat = model.predict(X_dummies)\n",
    "performance_before = model.evaluate(X_dummies, Y_dummies)\n",
    "performance_before\n",
    "\n",
    "importance = list()\n",
    "k=0\n",
    "for i in range(len(X_dummies)):\n",
    "    X_copy = np.copy(X)\n",
    "    variable = np.random.permutation(np.copy(X_copy[:,i]))\n",
    "\n",
    "    X_copy[:,i] = variable\n",
    "    \n",
    "    performance_after = model.evaluate(X_copy,Y_dummies)\n",
    "    \n",
    "    importance.append(performance_before - performance_after)\n",
    "    print(performance_before - performance_after)\n",
    "    k += 1\n",
    "    print(k)\n",
    "    if k >= 100:\n",
    "        break"
   ],
   "outputs": [],
   "metadata": {}
  }
 ],
 "metadata": {
  "orig_nbformat": 4,
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}